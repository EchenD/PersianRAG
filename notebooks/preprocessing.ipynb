{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "496cfd70-487b-416f-90ca-c4ffad8638dc",
   "metadata": {},
   "source": [
    "Load raw Persian text (e.g., enhelal.txt), normalize, clean, tokenize into sentences and paragraphs, and split into chunks suitable for embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39234790-8a2c-44be-923c-d8e2a41111c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 1) Attempt 1: If current working dir is 'notebooks', add parent folder\n",
    "notebooks_dir = os.getcwd()\n",
    "print(\"Notebook cwd before:\", notebooks_dir)\n",
    "\n",
    "# The code below computes candidate root: one level above notebooks.\n",
    "project_root_candidate = os.path.abspath(os.path.join(notebooks_dir, os.pardir))\n",
    "if os.path.isdir(os.path.join(project_root_candidate, \"modules\")):\n",
    "    if project_root_candidate not in sys.path:\n",
    "        sys.path.insert(0, project_root_candidate)\n",
    "        print(\"Inserted project root into sys.path:\", project_root_candidate)\n",
    "else:\n",
    "    # 2) Fallback: maybe cwd is already project root\n",
    "    if os.path.isdir(os.path.join(notebooks_dir, \"modules\")):\n",
    "        if notebooks_dir not in sys.path:\n",
    "            sys.path.insert(0, notebooks_dir)\n",
    "            print(\"Inserted notebooks_dir as project root into sys.path:\", notebooks_dir)\n",
    "\n",
    "# Confirm sys.path\n",
    "print(\"First entries of sys.path:\", sys.path[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929759e0-aa16-4dcb-a4b4-62756a4ff972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import re\n",
    "from hazm import Normalizer, word_tokenize, sent_tokenize\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Bring in GPU memory cleanup (import from Notebook 1 if needed)\n",
    "import torch\n",
    "\n",
    "# Import utility functions from modules/utils.py\n",
    "from modules.utils import clean_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f221ae0e-ea77-43a3-8947-ca1ffea7aad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust path to raw file\n",
    "raw_path = os.path.join(\"..\", \"data\" , \"docs\", \"enhelal.txt\")\n",
    "loader = TextLoader(raw_path, encoding=\"utf-8\")\n",
    "raw_docs = loader.load()\n",
    "print(f\"Loaded {len(raw_docs)} raw documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d56b4e1-a7d5-426f-90e1-1c0fecc43e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean each document\n",
    "cleaned_docs = [\n",
    "    Document(page_content=clean_text(doc.page_content), metadata=doc.metadata)\n",
    "    for doc in raw_docs\n",
    "]\n",
    "\n",
    "# Split into 3-sentence paragraphs\n",
    "paragraphs = []\n",
    "for d in cleaned_docs:\n",
    "    sentences = sent_tokenize(d.page_content)\n",
    "    for i in range(0, len(sentences), 3):\n",
    "        para = \" \".join(sentences[i:i + 3])\n",
    "        if para.strip():\n",
    "            paragraphs.append(Document(page_content=para, metadata=d.metadata))\n",
    "print(f\"Number of paragraphs: {len(paragraphs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5327dace-0fdd-4d71-b8c6-9065ae39a978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=30,\n",
    "    length_function=lambda x: len(x.split()),\n",
    "    separators=[\"۔\", \"؛\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents(paragraphs)\n",
    "# Filter out very short chunks (fewer than 20 words)\n",
    "chunks = [c for c in chunks if len(c.page_content.split()) > 20]\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk.metadata['chunk_index'] = i\n",
    "print(f\"Number of chunks after splitting: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed566a41-d411-4a7b-a033-c0417ca6241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, save `chunks` to disk (e.g., as JSON or pickle) for future loading\n",
    "import pickle\n",
    "\n",
    "with open(os.path.join(\"..\", \"data\", \"chunks.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(chunks, f)\n",
    "print(\"Chunks saved to data/chunks.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5aebc3-ed44-43d1-a1ea-790ee79f5c05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm-env-py311)",
   "language": "python",
   "name": "llm-env-py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
