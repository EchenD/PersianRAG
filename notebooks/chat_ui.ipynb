{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b7aa9fd-45cb-4293-a0c3-ed2d9c773fec",
   "metadata": {},
   "source": [
    "Assemble the RAG chat UI using ipywidgets, handling greeting detection, meta-questions, query rewriting, prompt building, streaming LLM responses, and final rendering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d4a3e96-8cfc-4040-b6e0-ed0a46337c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, uuid, re, time, json, torch\n",
    "import nbimporter\n",
    "\n",
    "from collections import deque\n",
    "from html import escape\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display, HTML\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2e65970-c60a-4007-be13-88eb9481c991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project root setup\n",
    "notebooks_dir = os.getcwd()\n",
    "project_root_candidate = os.path.abspath(os.path.join(notebooks_dir, os.pardir))\n",
    "if os.path.isdir(os.path.join(project_root_candidate, \"modules\")):\n",
    "    if project_root_candidate not in sys.path:\n",
    "        sys.path.insert(0, project_root_candidate)\n",
    "else:\n",
    "    if os.path.isdir(os.path.join(notebooks_dir, \"modules\")):\n",
    "        if notebooks_dir not in sys.path:\n",
    "            sys.path.insert(0, notebooks_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a48b05c1-9cdf-4275-8f53-b5fbe788dbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hamid\\miniconda3\\envs\\llm-env-py311\\Lib\\site-packages\\transformers\\utils\\hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from retriever_logic import getRetriever\n",
    "\n",
    "from modules.utils import sanitize_input, rewrite_user_query, build_context, token_is_valid, audit_response, log_interaction\n",
    "from modules.qa import handle_greeting, handle_meta_question\n",
    "from modules.model_manager import ModelManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2a44293-d2f4-400a-a81d-c88b9d3d823e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".message { margin: 5px 0; }\n",
       ".user { color: #2c3e50; }\n",
       ".assistant { color: #2980b9; }\n",
       ".typing { color: #2980b9; }\n",
       ".typing span { opacity: 0; animation: blink 1.5s infinite; color: #2980b9; }\n",
       ".typing span:nth-child(1) { animation-delay: 0s; }\n",
       ".typing span:nth-child(2) { animation-delay: 0.5s; }\n",
       ".typing span:nth-child(3) { animation-delay: 1s; }\n",
       "@keyframes blink { 0% { opacity: 0; } 50% { opacity: 1; } 100% { opacity: 0; } }\n",
       "#conversation { font-family: Tahoma, sans-serif; padding: 10px; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(\"\"\"\n",
    "<style>\n",
    ".message { margin: 5px 0; }\n",
    ".user { color: #2c3e50; }\n",
    ".assistant { color: #2980b9; }\n",
    ".typing { color: #2980b9; }\n",
    ".typing span { opacity: 0; animation: blink 1.5s infinite; color: #2980b9; }\n",
    ".typing span:nth-child(1) { animation-delay: 0s; }\n",
    ".typing span:nth-child(2) { animation-delay: 0.5s; }\n",
    ".typing span:nth-child(3) { animation-delay: 1s; }\n",
    "@keyframes blink { 0% { opacity: 0; } 50% { opacity: 1; } 100% { opacity: 0; } }\n",
    "#conversation { font-family: Tahoma, sans-serif; padding: 10px; }\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4338247f-b92c-49e0-8cd1-0f1a686b550b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('../data/models.json', 'r') as f:\n",
    "    models = json.load(f)\n",
    "\n",
    "model_manager = ModelManager()\n",
    "llm = None\n",
    "prompt_template_key = None\n",
    "model_dropdown = widgets.Dropdown(options=list(models.keys()), description='انتخاب مدل:', style={'description_width': 'initial'})\n",
    "\n",
    "def on_model_change(change):\n",
    "    global llm , prompt_template_key\n",
    "    llm = None\n",
    "    prompt_template_key = None\n",
    "    model_key           = change['new']\n",
    "    model_info          = models[model_key]\n",
    "    model_path          = model_info[\"path\"]\n",
    "    prompt_template_key = model_info[\"prompt_template_key\"];\n",
    "    params              = model_info.get(\"params\", {})\n",
    "    model_manager.load_model(model_path, model_key, **params)\n",
    "    llm = model_manager.get_current_model()\n",
    "\n",
    "\n",
    "model_dropdown.observe(on_model_change, names='value')\n",
    "initial_model = list(models.keys())[0]\n",
    "#on_model_change({'new': initial_model})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d75f636f-30ee-4689-bbd0-d0d3840002c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = deque(maxlen=100)\n",
    "stop_generation = False\n",
    "enable_thinking = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4759650-2298-489a-8a03-2fc26460733a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name HooshvareLab/bert-fa-base-uncased. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS retriever (k=100) ready.\n"
     ]
    }
   ],
   "source": [
    "def get_live_data():\n",
    "    import datetime\n",
    "    now = datetime.datetime.now()\n",
    "    time_str = now.strftime(\"%H:%M:%S\")\n",
    "    date_str = now.strftime(\"%Y-%m-%d\")\n",
    "    weather = \"آفتابی\"  # Placeholder\n",
    "    return f\"زمان: {time_str}، تاریخ: {date_str}، آب و هوا: {weather}\"\n",
    "\n",
    "retriever = getRetriever()\n",
    "def get_any_data(query: str):\n",
    "#    rewritten_query = rewrite_user_query(query, llm)\n",
    "    retrieved_docs = retriever.get_relevant_documents(query)\n",
    "    context_chunks, context_html = build_context(retrieved_docs)\n",
    "    retrieved_context.value = context_html\n",
    "    \n",
    "    context = \" \".join([doc.page_content for doc in retrieved_docs])\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "979909d9-9fec-4993-a531-618d160c8204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_assistant_response(response: str):\n",
    "    thinking = \"\"\n",
    "    tool_calls = []\n",
    "    final_answer = \"\"\n",
    "\n",
    "    think_match = re.search(r'<think>(.*?)</think>', response, re.DOTALL)\n",
    "    if think_match:\n",
    "        thinking = think_match.group(1).strip()\n",
    "\n",
    "    tool_call_pattern = r'<tool_call>\\s*(.*?)\\s*</tool_call>'\n",
    "    tool_call_matches = re.findall(tool_call_pattern, response, re.DOTALL)\n",
    "    for match in tool_call_matches:\n",
    "        try:\n",
    "            tool_call = json.loads(match)\n",
    "            tool_calls.append(tool_call)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "    parts = re.split(r'</think>|</tool_call>', response)\n",
    "    if parts:\n",
    "        final_answer = parts[-1].strip()\n",
    "\n",
    "    return thinking, tool_calls, final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22dc83e5-bbc1-4a5b-89bb-8dd7b218a387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_history(history: List[Dict[str, str]]) -> List[Dict[str, str]]:\n",
    "    flat_history: List[Dict[str, str]] = []\n",
    "    for turn in history:\n",
    "        role = turn.get(\"role\", \"\")\n",
    "        content = turn.get(\"content\", \"\")\n",
    "        if role == \"tool\":\n",
    "            wrapped = f\"<tool_response>\\n{content}\\n</tool_response>\"\n",
    "            flat_history.append({\"role\": \"user\", \"content\": wrapped})\n",
    "        \n",
    "        elif role in (\"system\", \"user\", \"assistant\"):\n",
    "            flat_history.append({\"role\": role, \"content\": content})\n",
    "        \n",
    "        else:\n",
    "            flat_history.append({\"role\": role, \"content\": content})\n",
    "    \n",
    "    return flat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "408e2499-332c-4a80-88d8-fd7fa7e79d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Persian Rag Assistant\"\n",
    "\n",
    "system_message = f\"\"\"\n",
    "Your name is {name}, a Retrieval-Augmented Generation (RAG) assistant. Follow these rules precisely:\n",
    "\n",
    "1. All user inputs will be in Persian. For each user message:\n",
    "   a. If it is a simple greeting (e.g., 'سلام', 'خوبی؟'), respond with a brief Persian greeting (e.g., 'سلام! چطور می‌توانم کمک کنم؟') and do NOT call any tool.\n",
    "   b. Otherwise, choose exactly one of the tools defined below and invoke it. Do NOT generate any other content.\n",
    "   c. If you cannot find a suitable tool or there is not enough information, reply exactly: 'نمی‌دونم.'\n",
    "2. OUTPUT RULE: All your final replies must be written fully in Persian (Persian script). Do NOT include any English words, Latin letters, or transliterations.\n",
    "\"\"\"\n",
    "\n",
    "history.append({\"role\": \"system\", \"content\": system_message})\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"name\": \"get_live_data\",\n",
    "        \"description\": \"\"\"\n",
    "        Retrieve live data such as:\n",
    "          • Current time and date\n",
    "          • Weather in a specified city\n",
    "        Examples (Persian → tool):\n",
    "          - 'الان ساعت چنده؟' → get_live_data\n",
    "          - 'هوا امروز توی تهران چه‌جوریه؟' → get_live_data\n",
    "        Non-examples:\n",
    "          - 'در فایل PDF بخش سوم را پیدا کن' → NOT get_live_data\n",
    "        \"\"\",\n",
    "        \"parameters\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Persian question asking for live data.\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_any_data\",\n",
    "        \"description\": \"\"\"\n",
    "        Retrieve data from uploaded documents or a knowledge base. Use this tool when:\n",
    "          • The user asks for content that must be looked up in a Persian document or index.\n",
    "        Examples (Persian → tool):\n",
    "          - 'در فایلِ PDF فصل سوم را پیدا کن و خلاصه‌اش را بگو.' → get_any_data\n",
    "          - 'لیست ایمیل شرکت را از فایل اکسل استخراج کن.' → get_any_data\n",
    "        Non-examples:\n",
    "          - 'الان تاریخ چنده؟' → NOT get_any_data\n",
    "        Parameters (if supporting multiple indices):\n",
    "          • index: name of the document index or database to search (optional)\n",
    "        \"\"\",\n",
    "        \"parameters\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Persian question for retrieving from documents.\"\n",
    "            },\n",
    "            \"index\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Optional: name of the document index or customer-specific database.\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79ae44ae-8467-4a4a-8829-005c3a8d03f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_templates(history: str) -> str:\n",
    "    template_dir = \"'../templates\"\n",
    "    env = Environment(loader=FileSystemLoader(template_dir), autoescape=False)\n",
    "    env.filters[\"tojson\"] = lambda value: json.dumps(value, sort_keys=False, ensure_ascii=False)\n",
    "    template = env.get_template(prompt_template_key)\n",
    "    context = {\"tools\": tools,\"messages\": flatten_history(history),\"add_generation_prompt\": True , \"enable_thinking\":enable_thinking}\n",
    "    rendered_output = template.render(context)\n",
    "    return rendered_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfc6439c-8c85-4ef6-a449-033b6292782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_conversation(history, is_typing=False, current_response=\"\"):\n",
    "    html = \"<div id='conversation' dir='rtl' lang='fa'>\"\n",
    "    for turn in history:\n",
    "        if turn['role'] == 'user':\n",
    "            html += f\"<div class='message user'><b>یوزر:</b> {escape(turn['content']).replace(chr(10), '<br>')}</div>\"\n",
    "        elif turn['role'] == 'assistant':\n",
    "            if enable_thinking and 'thinking' in turn:\n",
    "                html += f\"<div class='message assistant'><b>فکر کردن:</b> {escape(turn['thinking']).replace(chr(10), '<br>')}</div>\"\n",
    "            html += f\"<div class='message assistant'><b>اسیستنت:</b> {escape(turn['content']).replace(chr(10), '<br>')}</div>\"\n",
    "    if current_response:\n",
    "        html += f\"<div class='message assistant'><b>اسیستنت:</b> {escape(current_response).replace(chr(10), '<br>')}</div>\"\n",
    "    if is_typing and not current_response:\n",
    "        html += \"<div class='typing'>اسیستنت در حال فکر کردن<span>.</span><span>.</span><span>.</span></div>\"\n",
    "    html += \"</div>\"\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4556870e-dca4-43a8-af41-91cb6fc9688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_streaming(response, delay=0.1):\n",
    "    global stop_generation\n",
    "    words = response.split()\n",
    "    current_text = \"\"\n",
    "    for word in words:\n",
    "        if stop_generation:\n",
    "            current_text += \" [توقف شد]\"\n",
    "            break\n",
    "        current_text += word + \" \"\n",
    "        conversation_widget.value = format_conversation(history, current_response=current_text.strip())\n",
    "        time.sleep(delay)\n",
    "    return current_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e6154eb-7efd-4a77-af3f-446dd4c9dea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_submit(button):\n",
    "    global stop_generation\n",
    "    user_input = text_input.value.strip()\n",
    "    if not user_input:\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        user_question = sanitize_input(user_input)\n",
    "    except ValueError as e:\n",
    "        conversation_widget.value = format_conversation(history, current_response=f\"خطا: {e}\")\n",
    "        return\n",
    "\n",
    "    text_input.value = \"\"\n",
    "    history.append({\"role\": \"user\", \"content\": user_question})\n",
    "    conversation_widget.value = format_conversation(history, is_typing=True)\n",
    "\n",
    "    # greeting = handle_greeting(user_question)\n",
    "    # if greeting:\n",
    "    #     state, reply = greeting\n",
    "    #     final_response = simulate_streaming(reply)\n",
    "    #     history.append({'role': 'assistant', 'content': final_response})\n",
    "    #     conversation_widget.value = format_conversation(history)\n",
    "    #     return\n",
    "\n",
    "    # meta_question = handle_meta_question(user_question)\n",
    "    # if meta_question:\n",
    "    #     state, reply = meta_question\n",
    "    #     final_response = simulate_streaming(reply)\n",
    "    #     history.append({'role': 'assistant', 'content': final_response})\n",
    "    #     conversation_widget.value = format_conversation(history)\n",
    "    #     return\n",
    "\n",
    "    submit_button.disabled = True\n",
    "    stop_button.layout.display = ''\n",
    "    stop_generation = False\n",
    "\n",
    "    prompt = build_prompt_templates(history)\n",
    "\n",
    "    all_thinking = []\n",
    "    tool_iteration = 0\n",
    "    max_tool_iterations = 5\n",
    "\n",
    "    while tool_iteration < max_tool_iterations:\n",
    "        response = \"\"\n",
    "        try:\n",
    "            prompt_output.value = \"<b>پرامپت نهایی:</b><br>\" + escape(prompt).replace(chr(10), \"<br>\")\n",
    "            tool_open_tag  = \"<tool_call>\"\n",
    "            tool_close_tag = \"</tool_call>\"\n",
    "            visible_response = \"\"\n",
    "            tool_buffer      = \"\"\n",
    "            in_tool_call     = False\n",
    "            stream_buffer    = \"\"\n",
    "            for completion in llm.create_completion(\n",
    "                prompt=prompt,\n",
    "                max_tokens=512,\n",
    "                temperature=0.8,\n",
    "                top_p=0.95,\n",
    "                top_k=40,\n",
    "                repeat_penalty=1.1,\n",
    "                stream=True,\n",
    "                min_p=0,\n",
    "#                stop=[\"<|im_end|>\", \"\\n\"],  # Stop at end token or newline\n",
    "                ):\n",
    "                if stop_generation:\n",
    "                    response += \" [توقف شد]\"\n",
    "                    break\n",
    "                token = completion[\"choices\"][0][\"text\"]\n",
    "                stream_buffer += token\n",
    "                response += token\n",
    "                if not in_tool_call:\n",
    "                    idx = stream_buffer.find(tool_open_tag)\n",
    "                    if idx == -1:\n",
    "                        visible_response += stream_buffer\n",
    "                        stream_buffer = \"\"\n",
    "                        conversation_widget.value = format_conversation(history, current_response=visible_response)\n",
    "                    else:\n",
    "                        before_tag = stream_buffer[:idx]\n",
    "                        after_tag  = stream_buffer[idx:]\n",
    "                        visible_response += before_tag\n",
    "                        conversation_widget.value = format_conversation(history, current_response=visible_response)\n",
    "                        in_tool_call = True\n",
    "                        tool_buffer  = after_tag\n",
    "                        stream_buffer = \"\"\n",
    "                else:\n",
    "                    tool_buffer += stream_buffer\n",
    "                    stream_buffer = \"\"\n",
    "                    idx_close = tool_buffer.find(tool_close_tag)\n",
    "                    if idx_close != -1:\n",
    "                        full_block  = tool_buffer[: idx_close + len(tool_close_tag)]\n",
    "                        remainder   = tool_buffer[idx_close + len(tool_close_tag) :]\n",
    "                        json_text = re.sub(r\"^<tool_call>\\s*|\\s*</tool_call>$\", \"\", full_block)\n",
    "                        visible_response += remainder\n",
    "                        in_tool_call  = False\n",
    "                        tool_buffer   = \"\"\n",
    "                        conversation_widget.value = format_conversation(history, current_response=visible_response)\n",
    "                    else:\n",
    "                        conversation_widget.value = format_conversation(history, is_typing=True)\n",
    "                        pass\n",
    "            thinking, tool_calls, final_answer = parse_assistant_response(response)\n",
    "            if thinking:\n",
    "                all_thinking.append(thinking)\n",
    "            if tool_calls:\n",
    "                for tool_call in tool_calls:\n",
    "                    print(tool_call)\n",
    "                    tool_name = tool_call.get(\"name\")\n",
    "                    arguments = tool_call.get(\"arguments\", {})\n",
    "                    try:\n",
    "                        if tool_name == \"get_live_data\":\n",
    "                            tool_response = get_live_data()\n",
    "                        elif tool_name == \"get_any_data\":\n",
    "                            query = arguments.get(\"query\", \"\")\n",
    "                            tool_response = get_any_data(query)\n",
    "                        else:\n",
    "                            tool_response = \"the tool was not correctly called\"\n",
    "                        history.append({'role': 'tool', 'content': f\"{tool_response}\"})\n",
    "                    except Exception as e:\n",
    "                        history.append({\"role\": \"tool\", \"content\": f\"خطا در اجرای ابزار: {e}\"})\n",
    "                prompt = build_prompt_templates(history)\n",
    "                tool_iteration += 1\n",
    "            else:\n",
    "                history.append({'role': 'assistant', 'content': final_answer, 'thinking': \"\\n\".join(all_thinking)})\n",
    "                conversation_widget.value = format_conversation(history)\n",
    "                break\n",
    "        except Exception as e:\n",
    "            final_answer = f\"خطا: {e}\"\n",
    "            history.append({'role': 'assistant', 'content': final_answer, 'thinking': \"\\n\".join(all_thinking)})\n",
    "            break\n",
    "    else:\n",
    "        final_answer = \"حداکثر تعداد فراخوانی ابزار رسیده است.\"\n",
    "        history.append({'role': 'assistant', 'content': final_answer, 'thinking': \"\\n\".join(all_thinking)})\n",
    "        conversation_widget.value = format_conversation(history)\n",
    "\n",
    "    history[-1][\"thinking\"] = \"\\n\".join(all_thinking) if all_thinking else \"\"\n",
    "    history[-1][\"assistant\"] = final_answer\n",
    "    conversation_widget.value = format_conversation(history)\n",
    "\n",
    " #   if not final_answer.startswith(\"خطا\"):\n",
    " #       is_clean = audit_response(final_answer, context, llm)\n",
    " #   else:\n",
    " #       is_clean = False\n",
    "\n",
    "#    log_interaction(user_question, context, final_answer, is_clean)\n",
    "    submit_button.disabled = False\n",
    "    stop_button.layout.display = 'none'\n",
    "#    responce_output.value = \"<b>پاسخ نهایی:</b><br>\" + escape(final_answer).replace(chr(10), \"<br>\") if is_clean else \"خطا: پاسخ ممکن است نادرست باشد.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d19ebcb-1ac2-499e-aeaa-2c591ff88d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d08062d358c454abb54e2cc80bdf834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='', layout=Layout(height='300px', overflow='auto', padding='20px', width='99%')), HB…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "حافظه GPU پاک شد: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "def on_clear(button):\n",
    "    first_element = history[0]\n",
    "    history.clear()\n",
    "    history.append(first_element)\n",
    "    conversation_widget.value = format_conversation(history)\n",
    "    retrieved_context.value = \"\"\n",
    "    prompt_output.value = \"\"\n",
    "    responce_output.value = \"\"\n",
    "\n",
    "def on_stop(button):\n",
    "    global stop_generation\n",
    "    stop_generation = True\n",
    "\n",
    "def on_unload_model(button):\n",
    "    global llm , prompt_template_key\n",
    "    llm = None\n",
    "    prompt_template_key = None\n",
    "    model_manager.unload_model()\n",
    "\n",
    "conversation_widget = widgets.HTML(value=\"\", layout=widgets.Layout(width='99%', height='300px', overflow='auto', padding='20px'))\n",
    "retrieved_context = widgets.HTML(value=\"\", layout=widgets.Layout(width='99%', height='200px', overflow='auto', border='2px solid #ccc', padding='20px'))\n",
    "prompt_output = widgets.HTML(value=\"\", layout=widgets.Layout(width='99%', height='200px', overflow='auto', border='2px solid #ccc', padding='20px'))\n",
    "responce_output = widgets.HTML(value=\"\", layout=widgets.Layout(width='99%', height='100px', overflow='auto', border='2px solid #ccc', padding='20px'))\n",
    "\n",
    "text_input = widgets.Text(value='', placeholder='سوال خود را اینجا تایپ کنید', layout=widgets.Layout(width='70%', direction='rtl'))\n",
    "submit_button = widgets.Button(description=\"ارسال\", button_style='success')\n",
    "clear_button = widgets.Button(description=\"پاک کردن تاریخچه\", button_style='warning')\n",
    "stop_button = widgets.Button(description=\"توقف\", button_style='danger', layout=widgets.Layout(display='none'))\n",
    "unload_button = widgets.Button(description=\"X\", button_style='danger', layout=widgets.Layout(width='40px'))\n",
    "thinking_toggle = widgets.ToggleButton(value=True, description='نمایش فکر کردن', button_style='info')\n",
    "\n",
    "submit_button.on_click(on_submit)\n",
    "clear_button.on_click(on_clear)\n",
    "stop_button.on_click(on_stop)\n",
    "unload_button.on_click(on_unload_model)\n",
    "\n",
    "def on_thinking_toggle(change):\n",
    "    global enable_thinking\n",
    "    enable_thinking = change['new']\n",
    "    conversation_widget.value = format_conversation(history)\n",
    "\n",
    "thinking_toggle.observe(on_thinking_toggle, names='value')\n",
    "\n",
    "interface = widgets.VBox([\n",
    "    conversation_widget,\n",
    "    widgets.HBox([unload_button, model_dropdown, text_input, submit_button, clear_button, stop_button, thinking_toggle], layout=widgets.Layout(justify_content='flex-start', direction='rtl', width='99%')),\n",
    "    retrieved_context,\n",
    "    prompt_output,\n",
    "    responce_output\n",
    "])\n",
    "display(interface)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"حافظه GPU پاک شد: {torch.cuda.memory_allocated(0)/1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5120ac1c-35b9-46fb-9647-35a1bfa6c8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm-env-py311)",
   "language": "python",
   "name": "llm-env-py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
